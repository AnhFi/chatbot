import os, re, json
import pandas as pd
from unidecode import unidecode
from fuzzywuzzy import fuzz
from fastapi import FastAPI, Request
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

# ===== 0. H√ÄM H·ªñ TR·ª¢ =====
def remove_vietnamese_diacritics(text: str) -> str:
    return unidecode(text).lower()

def pretty(text: str) -> str:
    text = re.sub(r"([;:.\)\]\}])\s*(?=[\+\-‚Ä¢‚Äì])", r"\1\n", text)
    text = re.sub(r"\s*([+\-‚Ä¢‚Äì])\s+", r"\n\1 ", text)
    text = re.sub(r"\s*(ƒêi·ªÅu\s+\d+\.)", r"\n\1", text, flags=re.IGNORECASE)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

# ===== 1. ƒê∆Ø·ªúNG D·∫™N & THAM S·ªê =====
DB_DIR = "./vector_store"
COLLECTION = "so_tay_hcmue"
CHUNKS_TXT = "./chunks.txt"
TABLE_JSON = "./so_tay_all_tables_clean.json"
COURSE_JSON = "./mon_hoc_mo_ta.json"
MIN_EXPECTED_COUNT = 200
TOP_K = 5

# ===== 2. T·∫¢I D·ªÆ LI·ªÜU JSON =====
with open(TABLE_JSON, "r", encoding="utf-8") as f:
    tables = json.load(f)

with open(COURSE_JSON, "r", encoding="utf-8") as f:
    course_list = json.load(f)

COURSE_DATA = {
    remove_vietnamese_diacritics(item['ten_mon']): {
        "ten_mon": item['ten_mon'],
        "Description": item['Description']
    }
    for item in course_list
}

# ===== 3. KH·ªûI T·∫†O MODEL & CHROMA =====
print("üöÄ ƒêang t·∫£i model...")
model = SentenceTransformer("keepitreal/vietnamese-sbert")

print("üìö ƒêang kh·ªüi t·∫°o ChromaDB...")
client = chromadb.PersistentClient(path=DB_DIR, settings=Settings())
col = None

def ensure_collection_ready():
    """ƒê·∫£m b·∫£o collection t·ªìn t·∫°i ho·∫∑c t·∫°o m·ªõi t·ª´ chunks.txt"""
    global col
    try:
        col = client.get_collection(COLLECTION)
        total = col.count()
        if total >= MIN_EXPECTED_COUNT:
            return
    except Exception:
        pass

    os.makedirs(DB_DIR, exist_ok=True)
    try:
        client.delete_collection(COLLECTION)
    except Exception:
        pass
    col = client.create_collection(COLLECTION)

    with open(CHUNKS_TXT, "r", encoding="utf-8") as f:
        docs = [x.strip() for x in f.read().split("\n\n") if len(x.strip()) > 80][:400]  # Gi·ªõi h·∫°n nh·∫π RAM

    metadatas = [{"source": "So_Tay_Chinh"} for _ in docs]
    embs = []
    for i in range(0, len(docs), 32):
        batch = docs[i:i+32]
        batch_emb = model.encode(batch, normalize_embeddings=True).tolist()
        embs.extend(batch_emb)

    col.add(
        ids=[str(i) for i in range(len(docs))],
        documents=docs,
        embeddings=embs,
        metadatas=metadatas
    )

ensure_collection_ready()

# ===== 4. H√ÄM TRA C·ª®U =====
def find_table_by_keyword(query: str) -> str | None:
    normalized_query = remove_vietnamese_diacritics(query)
    mapping = {
        "4 sang chu": ["thang_diem_4"],
        "hoc bong": ["xep_loai_hoc_bong"],
        "chu sang 10": ["thang_diem_10_chu"],
        "xep loai hoc luc": ["xep_loai_hoc_luc"],
        "yeu cau hoc bong": ["yeu_cau_hoc_bong"],
        "diem ren luyen": ["diem_ren_luyen1", "diem_ren_luyen2", "diem_ren_luyen3"]
    }

    BEST_MATCH_SCORE = 90
    best_key, best_score = None, 0
    for k in mapping.keys():
        nk = remove_vietnamese_diacritics(k)
        score = max(
            fuzz.WRatio(normalized_query, nk),
            fuzz.partial_ratio(normalized_query, nk),
            fuzz.token_set_ratio(normalized_query, nk)
        )
        if score > best_score and score >= BEST_MATCH_SCORE:
            best_key, best_score = k, score

    if not best_key:
        return None

    out = [f"### üìä B·∫£ng tra c·ª©u (ƒê·ªô kh·ªõp: {best_score}%)"]
    for type_name in mapping[best_key]:
        for t in tables:
            if t.get("type") == type_name:
                df = pd.DataFrame(t["data"])
                title = t.get("title", type_name)
                out.append(f"\n#### {title}\n" + df.to_markdown(index=False))
    return "\n".join(out) if len(out) > 1 else None

def find_course_by_fuzzy_match(query: str):
    qn = remove_vietnamese_diacritics(query)
    best_score, best = 0, None
    for k, c in COURSE_DATA.items():
        score = max(
            fuzz.token_set_ratio(qn, k),
            fuzz.WRatio(qn, k),
            fuzz.partial_ratio(qn, k)
        )
        if score > best_score:
            best_score, best = score, c
    if best and best_score >= 90:
        return f"üìö **M√¥n h·ªçc:** {best['ten_mon']}\n\n{best['Description']}\n_(ƒê·ªô kh·ªõp: {best_score}%)_"
    return None

def query_vector_db(query: str):
    q_emb = model.encode(query, normalize_embeddings=True).tolist()
    res = col.query(
        query_embeddings=[q_emb],
        include=["documents", "metadatas"],
        where={"source": {"$eq": "So_Tay_Chinh"}},
        n_results=TOP_K
    )
    docs = (res.get("documents") or [[]])[0]
    return [pretty(d) for d in docs if d.strip()]

# ===== 5. KH·ªûI T·∫†O FASTAPI =====
app = FastAPI(title="üéì Chatbot Tra c·ª©u HCMUE", description="API t√¨m ki·∫øm n·ªôi dung S·ªï tay & M√¥n h·ªçc", version="1.0")

@app.get("/")
def root():
    return {"message": "‚úÖ Chatbot HCMUE API ƒëang ho·∫°t ƒë·ªông!"}

@app.post("/query")
async def query_api(request: Request):
    data = await request.json()
    query = data.get("question", "").strip()
    if not query:
        return {"error": "‚ö†Ô∏è Thi·∫øu tr∆∞·ªùng 'question'."}

    # 1) Tra b·∫£ng
    tb = find_table_by_keyword(query)
    if tb:
        return {"type": "table", "result": tb}

    # 2) Tra m√¥n h·ªçc
    course_rs = find_course_by_fuzzy_match(query)
    if course_rs:
        return {"type": "course", "result": course_rs}

    # 3) Vector search
    docs = query_vector_db(query)
    if not docs:
        return {"type": "none", "result": "‚ùå Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p."}

    return {"type": "vector", "result": docs}

# ===== 6. CH·∫†Y APP =====
if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
