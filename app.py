import os, re, json
import pandas as pd
from unidecode import unidecode
from fuzzywuzzy import fuzz
import gradio as gr

# ========== Embedding & Vector DB ==========
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer, util

# ===== 0. HÃ€M Há»– TRá»¢ =====
def remove_vietnamese_diacritics(text: str) -> str:
    return unidecode(text).lower()

def pretty(text: str) -> str:
    text = re.sub(r"([;:.\)\]\}])\s*(?=[\+\-â€¢â€“])", r"\1\n", text)
    text = re.sub(r"\s*([+\-â€¢â€“])\s+", r"\n\1 ", text)
    text = re.sub(r"\s*(Äiá»u\s+\d+\.)", r"\n\1", text, flags=re.IGNORECASE)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

# ===== 1. ÄÆ¯á»œNG DáºªN & THAM Sá» =====
DB_DIR = "./vector_store"              # nÆ¡i chá»©a ChromaDB persistent
COLLECTION = "so_tay_hcmue"
CHUNKS_TXT = "./chunks.txt"
TABLE_JSON = "./so_tay_all_tables_clean.json"
COURSE_JSON = "./mon_hoc_mo_ta.json"
MIN_EXPECTED_COUNT = 200               # ngÆ°á»¡ng sanity check
TOP_K = 5

# ===== 2. Táº¢I Dá»® LIá»†U JSON =====
with open(TABLE_JSON, "r", encoding="utf-8") as f:
    tables = json.load(f)

with open(COURSE_JSON, "r", encoding="utf-8") as f:
    course_list = json.load(f)

COURSE_DATA = {
    remove_vietnamese_diacritics(item['ten_mon']): {
        "ten_mon": item['ten_mon'],
        "Description": item['Description']
    }
    for item in course_list
}

# ===== 3. KHá»žI Táº O MODEL & CHROMA =====
model = SentenceTransformer("keepitreal/vietnamese-sbert")

# chroma client
client = chromadb.PersistentClient(path=DB_DIR, settings=Settings())
col = None

def ensure_collection_ready():
    """Äáº£m báº£o collection tá»“n táº¡i. Náº¿u chÆ°a cÃ³, build tá»« chunks.txt"""
    global col
    try:
        col = client.get_collection(COLLECTION)
        # náº¿u cÃ³ rá»“i thÃ¬ kiá»ƒm tra sá»‘ lÆ°á»£ng
        total = col.count()
        if total >= MIN_EXPECTED_COUNT:
            return
    except Exception:
        pass

    # Náº¿u chÆ°a cÃ³ hoáº·c thiáº¿u â†’ build má»›i
    # Thá»­ láº¥y zip (náº¿u user upload vector_store.zip) â†’ giáº£i nÃ©n
    if not os.path.exists(DB_DIR) and os.path.exists("./vector_store.zip"):
        import zipfile
        with zipfile.ZipFile("./vector_store.zip", "r") as z:
            z.extractall("./")
    # thá»­ láº¡i get_collection
    try:
        col = client.get_collection(COLLECTION)
        if col.count() >= MIN_EXPECTED_COUNT:
            return
    except Exception:
        pass

    # KhÃ´ng cÃ³ sáºµn: build tá»« chunks.txt
    # (Ä‘áº£m báº£o DB_DIR tá»“n táº¡i)
    os.makedirs(DB_DIR, exist_ok=True)

    # reset instance & táº¡o má»›i
    try:
        chromadb.api.client.SharedSystemClient._instance = None
    except Exception:
        pass

    try:
        # xÃ³a náº¿u Ä‘Ã£ tá»“n táº¡i
        client.delete_collection(COLLECTION)
    except Exception:
        pass

    col = client.create_collection(COLLECTION)

    # Ä‘á»c chunks
    with open(CHUNKS_TXT, "r", encoding="utf-8") as f:
        docs = [x.strip() for x in f.read().split("\n\n") if len(x.strip()) > 80]

    if not docs:
        raise RuntimeError("KhÃ´ng cÃ³ dá»¯ liá»‡u trong chunks.txt Ä‘á»ƒ build vector store.")

    # Táº¡o metadata
    metadatas = [{"source": "So_Tay_Chinh"} for _ in docs]

    # Encode & add (batch)
    embs = []
    batch_size = 32
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i+batch_size]
        batch_emb = model.encode(batch, normalize_embeddings=True).tolist()
        embs.extend(batch_emb)

    col.add(
        ids=[str(i) for i in range(len(docs))],
        documents=docs,
        embeddings=embs,
        metadatas=metadatas
    )

ensure_collection_ready()

# ===== 4. HÃ€M TRA Cá»¨U Báº¢NG (mapping nhanh) =====
def find_table_by_keyword(query: str) -> str | None:
    normalized_query = remove_vietnamese_diacritics(query)

    mapping = {
        "4 sang chu": ["thang_diem_4"],
        "hoc bong": ["xep_loai_hoc_bong"],
        "chu sang 10": ["thang_diem_10_chu"],
        "thang diem 10 sang chu": ["thang_diem_10_chu"],
        "thang diem 4 sang chu": ["thang_diem_4"],
        "xep loai hoc luc": ["xep_loai_hoc_luc"],
        "xep loai hoc bong": ["xep_loai_hoc_bong"],
        "yeu cau hoc bong": ["yeu_cau_hoc_bong"],
        "diem ren luyen": ["diem_ren_luyen1", "diem_ren_luyen2", "diem_ren_luyen3", "diem_ren_luyen4", "diem_ren_luyen5"],
    }

    BEST_MATCH_SCORE = 90
    best_key, best_score = None, 0

    for k in mapping.keys():
        nk = remove_vietnamese_diacritics(k)
        score = max(
            fuzz.WRatio(normalized_query, nk),
            fuzz.partial_ratio(normalized_query, nk),
            fuzz.token_set_ratio(normalized_query, nk)
        )
        if score > best_score and score >= BEST_MATCH_SCORE:
            best_key, best_score = k, score

    if not best_key:
        return None

    # Xuáº¥t cÃ¡c báº£ng tÆ°Æ¡ng á»©ng
    out = [f"### ðŸ“Š Káº¿t quáº£ Tra cá»©u Báº£ng (Äá»™ khá»›p: {best_score}%)"]
    for type_name in mapping[best_key]:
        for t in tables:
            if t.get("type") == type_name:
                df = pd.DataFrame(t["data"])
                title = t.get("title", type_name.replace("_", " ").title())
                out.append(f"\n#### {title}\n" + df.to_markdown(index=False))
    return "\n".join(out) if len(out) > 1 else None

# ===== 5. Fuzzy match mÃ´n há»c =====
def find_course_by_fuzzy_match(query: str):
    qn = remove_vietnamese_diacritics(query)
    best_score, best = 0, None
    for k, c in COURSE_DATA.items():
        score = max(
            fuzz.token_set_ratio(qn, k),
            fuzz.WRatio(qn, k),
            fuzz.partial_ratio(qn, k)
        )
        if score > best_score:
            best_score, best = score, c
    if best and best_score >= 90:
        return f"ðŸ“š **MÃ´n há»c:** {best['ten_mon']}\n\n{best['Description']}\n\n_(Äá»™ khá»›p: {best_score}%)_"
    return None

# ===== 6. HÃ€M CHATBOT CHÃNH =====
def chatbot(query: str) -> str:
    if not query or len(remove_vietnamese_diacritics(query)) < 4:
        return "âš ï¸ Vui lÃ²ng nháº­p cÃ¢u há»i rÃµ hÆ¡n."

    # 1) Tra báº£ng trÆ°á»›c (náº¿u cÃ³)
    tb = find_table_by_keyword(query)
    if tb:
        return tb

    # 2) Thá»­ fuzzy mÃ´n há»c
    course_rs = find_course_by_fuzzy_match(query)
    if course_rs:
        return course_rs

    # 3) Vector search tá»« Sá»• tay
    try:
        q_emb = model.encode(query, normalize_embeddings=True).tolist()
        res = col.query(
            query_embeddings=[q_emb],
            include=["documents", "metadatas"],
            where={"source": {"$eq": "So_Tay_Chinh"}},
            n_results=TOP_K
        )
    except Exception as e:
        return f"âŒ Lá»—i truy váº¥n: {e}"

    docs = (res.get("documents") or [[]])[0]
    metas = (res.get("metadatas") or [[]])[0]

    if not docs:
        return "âŒ KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p."

    out = ["ðŸ“˜ **Káº¿t quáº£ tá»« Sá»• tay Sinh viÃªn:**\n"]
    for i, d in enumerate(docs):
        if d and d.strip():
            src = (metas[i] or {}).get("source", "So_Tay")
            out.append(f"**Äoáº¡n {i+1}** _(Nguá»“n: {src})_\n{pretty(d)}\n")
    return "\n".join(out)

# ===== 7. GIAO DIá»†N GRADIO =====
demo = gr.Interface(
    fn=chatbot,
    inputs=gr.Textbox(label="Nháº­p cÃ¢u há»i", placeholder="VD: Äiá»u kiá»‡n nháº­n há»c bá»•ng? Thang Ä‘iá»ƒm 4 sang chá»¯?"),
    outputs="markdown",
    title="ðŸŽ“ Chatbot Tra cá»©u HCMUE",
    description="Há»i vá» há»c pháº§n hoáº·c ná»™i dung Sá»• tay. Há»‡ thá»‘ng dÃ¹ng BGE-M3 + ChromaDB Ä‘á»ƒ truy váº¥n."
)

if __name__ == "__main__":
    import os
    port = int(os.environ.get("PORT", 7860))
    demo.launch(server_name="0.0.0.0", server_port=port)